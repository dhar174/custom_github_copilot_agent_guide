# Writing Effective GitHub Copilot Custom Agent Configuration Files

GitHub Copilot **Custom Agents** allow you to define AI assistants tailored to your project’s needs, living as Markdown configuration files in your repository. By creating custom agent profiles, you can embed your project’s structure, coding standards, and workflows directly into Copilot. This guide provides a comprehensive walkthrough for AI developers on crafting effective custom agent configs in the .github/agents/ directory. We’ll cover general principles and dive into specifics for both **orchestrator agents** (which coordinate tasks and delegate to others) and **specialist subagents** (which focus on specific domains or phases). Throughout, we reference GitHub’s official documentation and real-world examples to illustrate best practices.

## Structure of a Custom Agent Profile

Each custom agent is defined by a single Markdown file (commonly with a name like agent-name.agent.md) in your repo’s .github/agents/ folder. The file has two parts:

* **YAML Frontmatter (top section)** – Defines metadata about the agent (name, description, tools, etc.).

* **Markdown Body (below the frontmatter)** – Contains the prompt instructions that shape the agent’s behavior, written in natural language with Markdown formatting.

In essence, *agent profiles are Markdown files with a YAML header and a prompt body*[\[1\]](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents#:~:text=Agent%20profile%20format). According to GitHub Docs, an agent profile typically includes: a **name** (unique agent ID), a **description** (what the agent does), the **prompt** (instructions defining its behavior), and an optional **tools** list (which tools it can use)[\[2\]](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents#:~:text=Agent%20profiles%20are%20Markdown%20files,their%20simplest%20form%2C%20they%20include). Additional fields can fine-tune the agent’s usage and context.

### YAML Frontmatter Fields

The YAML frontmatter appears at the very top enclosed by triple-dash \--- markers. Here you configure core settings for the agent. Key fields include:

* **name** – Unique identifier for the agent. If omitted, the file name (without the extension) is used as default[\[3\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=Metadata%20Field%20Description%20,the%20filename%20if%20not%20specified). Choose a short, descriptive name (e.g. "lead-planner", "test-specialist") with no spaces. This name is how you’ll reference or invoke the agent.

* **description** – A one-line summary of the agent’s role and expertise. This is required and is shown in UI menus as the agent’s blurb[\[3\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=Metadata%20Field%20Description%20,the%20filename%20if%20not%20specified). For example: description: Focuses on test coverage and quality without modifying production code[\[4\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,without%20modifying%20production%20code). Keep it concise but explanatory.

* **target** – *(Optional)* Limits where the agent can run. Options are "github-copilot" (GitHub web/CLI environments) or "vscode" (VS Code and IDEs)[\[5\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=capabilities%20,true). If not set, the agent is available in both. Use this if you want an agent to be specific to one environment. (In our examples, agents are often set to target: github-copilot to indicate they’re for GitHub’s context.)[\[5\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=capabilities%20,true)

* **tools** – *(Optional)* An allow-list of tools the agent can use. By default, if you omit this field, the agent has access to **all** available tools[\[6\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=%60github,used%20by%20the%20custom%20agent)[\[7\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=,prefixed%20with%20the%20server%20name) – including file read/write, search, terminal execution, and any connected Model Context Protocol (MCP) tools. However, it’s a best practice to **limit tools to only what the agent needs**. Specify a YAML list of tool names/aliases, for example: tools: \["read", "search", "edit"\]. This would restrict the agent to reading files, searching, and editing, while disallowing others. You can also include all tools from a certain category via wildcards (e.g. tools: \["github/\*"\] to enable all GitHub MCP server tools)[\[8\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=available%20tool%20aliases%2C%20see%20Tool,tool)[\[9\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=The%20following%20MCP%20servers%20are,can%20be%20referenced%20using%20namespacing). All unrecognized tool names are ignored[\[10\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=All%20unrecognized%20tool%20names%20are,agent%20profile%20without%20causing%20problems). In summary: if no tools are specified, everything is enabled; if a list is provided, only those are enabled; an empty list \[\] would disable all tools[\[11\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=The%20,depends%20on%20what%20you%20specify). We’ll discuss tool selection strategies in detail later (“Scoped Tool Usage”).

* **infer** – *(Optional, boolean)* Controls automatic agent selection. When infer: true, Copilot may automatically invoke this agent in relevant contexts; when false, it will **only** be used if the user explicitly selects it[\[12\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=Supports%20both%20a%20comma%20separated,true). By default this is true (auto-selection allowed)[\[13\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=,should%20be%20used%20by%20the). For most custom agents in a multi-agent setup, you’ll set infer: false so that the orchestrator or user triggers them intentionally. This prevents Copilot from guessing incorrectly and ensures you have manual control over when the agent is used.

* **metadata** – *(Optional)* Arbitrary key-value pairs to annotate the agent[\[14\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=custom%20agent.%20,the%20agent%20with%20useful%20data). This doesn’t affect the agent’s functionality directly, but is useful for documentation or programmatically identifying agents. You can include things like project name, role, phase number, etc. For example, an orchestrator might have:

* metadata:  
    project: "MyProject"  
    role: "lead"  
    scope: "all"

* and a subagent might have:

* metadata:  
    project: "MyProject"  
    role: "backend"  
    phase: "2"

* In the provided examples, the **LNF** orchestrator’s metadata labels it as the lead role for project LNF with scope “all”[\[15\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=metadata%3A%20project%3A%20,), and the notebook subagent is tagged with its role and phase number[\[16\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=metadata%3A%20project%3A%20,4). Use metadata to convey any additional context that maintainers or orchestrator logic might find helpful (for instance, an orchestrator could read subagents’ metadata to decide assignments, or you might simply use it to keep profiles organized).

Other fields you might encounter include **mcp-servers** (for defining MCP server connections in org/enterprise-level agents) and **model** (to specify an AI model). **Important:** *GitHub.com’s Copilot (as of this writing) ignores model, argument-hint, and handoffs in the profile*[\[17\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=Nota%3A) – those are primarily for IDE integrations. In a repository-level agent on GitHub, you generally won’t include model or handoffs in the YAML, as they won’t have effect in the web/CLI environment. Instead, focus on the fields listed above.

Here’s a simplified example of what YAML frontmatter might look like for an orchestrator and a subagent:

\# Orchestrator agent frontmatter example  
\---  
name: project-lead  
description: Coordinates multi-agent tasks; delegates work and enforces project standards.  
target: github-copilot  
infer: false  
tools: \["agent", "read", "search", "edit", "execute"\]  \# allow calling subagents, reading, editing, running tests  
metadata:  
  project: "MyProject"  
  role: "lead"  
  scope: "all"  
\---

\# Subagent agent frontmatter example  
\---  
name: project-backend  
description: Handles backend module implementation (database models and API logic).  
target: github-copilot  
infer: false  
tools: \["read", "edit", "execute"\]  \# this agent can read code, write code, and run code (for tests)  
metadata:  
  project: "MyProject"  
  role: "backend"  
  phase: "2"  
\---

**Note:** The name is often repeated as part of the file name (e.g. project-backend.agent.md) for clarity, but the .agent.md suffix isn’t strictly required. The file just needs to reside in .github/agents/ and contain the YAML header; GitHub will recognize it as a custom agent profile[\[18\]](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents#:~:text=Where%20you%20can%20configure%20custom,agents).

### Prompt Instructions (Markdown Body)

After the YAML frontmatter, include a blank line then begin the **Markdown content** that constitutes the agent’s instructions or “persona.” This section can be as detailed as needed (up to \~30k characters)[\[19\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=annotation%20of%20the%20agent%20with,useful%20data). Here, you effectively program the agent’s behavior and scope using natural language guidelines. Writing this prompt is where you **encode the agent’s role, responsibilities, workflow, and rules.**

General tips for writing the prompt section:

* **Clearly define the agent’s identity and scope in the first lines.** For example: *“You are a documentation specialist focused on README files. Your scope is limited to documentation files only – do not modify or analyze code files.”*[\[20\]](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents#:~:text=You%20are%20a%20documentation%20specialist,modify%20or%20analyze%20code%20files). This opening establishes what the agent should (and should not) do. In an orchestrator, the opening might say *“You are the technical lead for the project, responsible for breaking down requests and delegating to specialist agents.”* In a specialist, it might say *“You are a testing expert who writes and improves test cases, without changing production code.”* Clearly stating the role sets the tone for all further behavior.

* **Use headings, bullet points, and formatting to structure the instructions.** A well-structured prompt is easier for both you and the AI to follow. Utilize Markdown headings (\#\#, \#\#\#) and lists for different sections of the agent’s mandate. For example, you might have sections like **“Primary Goals”**, **“Responsibilities”**, **“Scope”**, **“Constraints”**, **“Workflow”**, **“Standards”**, etc. The provided orchestrator profile uses multiple sections: “Primary goals,” “Primary Responsibilities,” “How to Delegate,” “Workflow for Implementation,” “Repo Conventions,” “Working style,” and “Definition of done,” each with bullet points detailing expectations[\[21\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Primary%20goals%3A%20,webui)[\[22\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=python%20scripts%2Fdemo_retrieval,in%20Colab%20and%20local%20dev). Such organization not only makes the file readable but also helps the AI internalize structured rules.

* **Be specific about tasks and avoid ambiguity.** List the types of tasks the agent should focus on, and if applicable, tasks it should **avoid**. In the README creator example, the prompt explicitly says to focus on README files and not to analyze code[\[20\]](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents#:~:text=You%20are%20a%20documentation%20specialist,modify%20or%20analyze%20code%20files). In a test-writing agent, you might instruct it to focus only on test files and not alter application code[\[23\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,production%20code%20unless%20specifically%20requested). If the agent should always follow certain steps or include certain sections in its output, spell that out.

* **Include examples or templates if helpful.** For complex workflows, it can help to provide a brief example of what the agent might do. For instance, an orchestrator might show a sample delegation command or a snippet of how to call a subagent (e.g., *“Example: If the user asks for X, delegate to the frontend agent with prompt Y”*[\[24\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Example%20Delegation%20Prompt%3A%20,and%20follows%20the%20project%20patterns)). This gives the AI a pattern to imitate. Similarly, you could include a small table mapping inputs to outputs or tasks to agents, which serves as a routing reference[\[25\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,).

* **Reinforce any critical behavioral rules.** If there are “must follow” rules (for example, “Never deploy or run destructive commands” or “Always follow PEP8 style in code”), make them prominent. You can label a section **Critical Rules** or **Constraints**. The LNF orchestrator example explicitly calls out in bold that it must not pretend to be another agent and must use the agent tool for delegation[\[26\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=How%20to%20Delegate%20%28CRITICAL%29%20,agent%20tool%20to%20assign%20work). Don’t hesitate to use emphasis (bold/italic) or call something **IMPORTANT:** to draw attention.

* **Address standards and completion criteria.** Especially for coding agents, you should embed your project’s definition of “done” and coding standards. An orchestrator agent often enforces these by checking the work of subagents. In its profile, list the key conventions (directory structure, coding style, testing requirements, documentation standards, etc.) that all outputs must adhere to[\[27\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Repo%20Conventions%20,in%20Colab%20and%20local%20dev). You can also define the **Definition of Done** as a checklist of what constitutes a successful task completion[\[28\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Working%20style%3A%20,basic%20lint%2Fformat%20checks%20if%20available). This helps the agent (and its subagents) know the target for quality. For a specialist agent, “Quality Gates” or acceptance criteria can be included – e.g., “All new code must pass existing tests”, “The produced notebook should open without errors”[\[29\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Quality%20gates%3A%20,where%20feasible), “The feature should be documented in CHANGELOG”, etc.

Think of the prompt content as *programming the agent with guidelines*. It’s a mix of role description, operating procedure, and rules of engagement. Next, we’ll explore how to apply this when designing orchestrator vs specialist agents.

## Orchestrator Agents (Multi-Agent Orchestration)

**Orchestrator agents** (sometimes called “manager” or “lead” agents) are custom agents designed to coordinate work across multiple subagents. They typically do not produce a final product themselves; instead, they break down complex tasks into smaller subtasks and delegate those to the appropriate specialist agents. An orchestrator is like the project manager or team lead in your AI agent team.

**Key responsibilities of an orchestrator agent:**

* **Task Analysis & Delegation:** The orchestrator examines the user’s request or a high-level goal and decides which specialized agent(s) should handle each part. It acts as a router, mapping each subtask to the best subagent for the job[\[30\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Primary%20Responsibilities%20,tests%20to%20confirm%20it%20works). For example, if a user asks for a new feature that involves database and frontend changes, the orchestrator might delegate database changes to a “db-agent” and frontend work to a “ui-agent”. The agent profile should include guidance on this mapping. One effective approach is to maintain a **table or list of subagent capabilities**. For instance, the LNF lead agent’s prompt has a table mapping **Task Type** to **Agent Name** with examples[\[25\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,), and a follow-up list enumerating when to call each agent[\[31\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=When%20to%20delegate%3A%20,webui). Your orchestrator’s instructions can similarly enumerate: “For documentation tasks, use docs-agent; for API changes, use backend-agent; for UI changes, use frontend-agent; for testing, use test-agent,” etc. This ensures deterministic routing – the orchestrator knows exactly which agent name to invoke for a given scenario.

* **Delegation Mechanics:** In Copilot, an orchestrator uses the **agent tool** to invoke a subagent (this tool may also appear as alias custom-agent or Task in some contexts[\[32\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=,and%20performing%20a%20web%20search)). The orchestrator profile **must emphasize** that it should *never simply assume the role of another agent or answer on their behalf*. Instead, it should call the subagent via the tool. The LNF orchestrator’s profile highlights this: *“You cannot 'become' another agent. You must invoke the agent tool to assign work.”*[\[26\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=How%20to%20Delegate%20%28CRITICAL%29%20,agent%20tool%20to%20assign%20work). Include a similar **Critical Delegation Rule** in your orchestrator prompt. For example: **“Never solve specialized tasks yourself if a subagent exists for them – always use the /agent \<name\> tool to delegate.”** You can even provide a short example of using the agent tool in context. The syntax will depend on interface (in CLI, you might do /agent backend-agent "Generate X", whereas in the GitHub web chat, the orchestrator can internally call the agent tool). While you don’t need to specify exact syntax in the prompt, describing the action (“call the \<agent-name\> agent with the following prompt…”) is helpful[\[24\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Example%20Delegation%20Prompt%3A%20,and%20follows%20the%20project%20patterns).

* **Aggregating and Context Passing:** After delegation, the orchestrator waits for the subagent to produce a result (code, text, etc.). It should then integrate or summarize those results as needed. Often, the orchestrator will **read the files or outputs** created by subagents using the read tool, to verify correctness or gather info for the next steps[\[33\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,py). In your prompt, outline what the orchestrator should do post-delegation. For example: *“After the subagent completes, read the changed files and verify they meet the acceptance criteria. If not, provide feedback and call the subagent again.”*[\[34\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,a%20corresponding%20test%20in%20tests) This encourages an iterative loop where the orchestrator checks subagents’ work and either approves it or asks for fixes (possibly by invoking the subagent again with a refined prompt).

* **Standard Enforcement & Review:** The orchestrator acts as the quality gatekeeper. Instruct it to enforce coding standards and project conventions on all contributions[\[35\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,tests%20to%20confirm%20it%20works)[\[27\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Repo%20Conventions%20,in%20Colab%20and%20local%20dev). For example, your orchestrator prompt might have a section like **“Repository Conventions (must enforce)”** listing rules (coding style, file layout, dependency guidelines, etc.). When reviewing a subagent’s output, the orchestrator should compare against these rules. If something is off (missing tests, improper naming, etc.), it should either make small edits (if allowed) or send the subagent back for corrections. In LNF’s orchestrator profile, there’s a clear mandate to enforce pure Python code, a certain project layout, use of Pydantic models, tests for each feature, etc.[\[36\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,in%20Colab%20and%20local%20dev). Tailor this to your project’s needs. This ensures the multi-agent output remains consistent and high-quality.

* **Final Integration & Testing:** Usually, the orchestrator is responsible for the final “assembly” of outputs and verifying everything works together. Make sure the orchestrator has access to necessary tools like execute (to run build or test commands) if this is part of your workflow. In the prompt, you can specify a step to run tests or a build script using the execute tool as a final validation[\[34\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,a%20corresponding%20test%20in%20tests). For example: *“Final Step: run npm test (via execute tool) to ensure all tests pass.”* And *“If tests fail, instruct the relevant subagent to fix the issues.”* By automating this check, the orchestrator effectively serves as an AI project reviewer. It can catch problems introduced by subagents and loop back to fix them.

**Tool configuration for orchestrators:** Orchestrators typically need a broad set of tools because they oversee many aspects. At minimum, an orchestrator should include the agent tool in its tools list (otherwise it cannot call subagents\!)[\[32\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=,and%20performing%20a%20web%20search). It’s also common to give orchestrators read/write capabilities and execution rights, since they may read files, make minor edits, or run tests. For example, the orchestrator’s tools might be \["agent", "read", "search", "edit", "execute", "github/\*"\]. This enables delegation, file access, editing, searching, running shell commands, and any GitHub MCP tools (like PR creation, etc.) if needed. In contrast, you might **omit** tools like web if you don’t want the orchestrator browsing the web, and certainly the orchestrator doesn’t usually need specialized tools that subagents would use (like it wouldn’t need a design tool specific to VS Code, etc.).

*Design decision:* Some teams choose to make the orchestrator **hands-off with code** (pure delegator), meaning they do **not** allow it to use edit or other write tools – it must delegate any code changes to subagents. This can be achieved by not listing edit/write in its tools (or explicitly denying them in some configs[\[37\]](https://gist.github.com/gc-victor/1d3eeb46ddfda5257c08744972e0fc4c#:~:text=%23%20Delegation%20,TOOL%20task%3A%20true)). This approach forces a strict separation of duties: the orchestrator only plans and reviews. Other teams allow the orchestrator some editing ability for trivial fixes or integrating outputs. For example, an orchestrator might accept suggestions from multiple subagents and then integrate them into one coherent change. If you trust the orchestrator to make minor adjustments, including edit is fine; if not, exclude it and let the specialists handle all code modifications. Either way, make sure the **prompt reflects the policy**: e.g. *“Do not attempt to implement features yourself – always delegate to a specialist agent for actual code changes.”*[\[38\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,Review%20%26%20Repair).

**Prompt structure for orchestrator:** We touched on parts of this above, but to summarize a possible layout inside the orchestrator’s Markdown body:

* A brief role description (e.g. “You are the lead AI agent coordinating all work on Project X.”).

* **Primary Goals** – bullet points of high-level goals (ensure plan followed, break into small tasks, meet acceptance criteria, etc.)[\[21\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Primary%20goals%3A%20,webui).

* **Primary Responsibilities** – bullet points like “Orchestration (delegating tasks)”, “Standard Enforcement (code and style checks)”, “Verification (testing and review)”[\[39\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,agent%20tool%20to%20assign%20work).

* **How to Delegate** – a highlighted rule that the agent must use the agent tool, not switch modes itself[\[26\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=How%20to%20Delegate%20%28CRITICAL%29%20,agent%20tool%20to%20assign%20work).

* **Mapping of Agents** – a table or list mapping task categories to subagent names[\[25\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,), possibly with examples of how to phrase the delegation prompt[\[24\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Example%20Delegation%20Prompt%3A%20,and%20follows%20the%20project%20patterns).

* **Workflow/Phases** – an ordered list of steps the orchestrator should follow, if applicable. For example, a common structure is: **Analyze** (understand the request and context, maybe by reading a planning document), **Plan** (determine which agents and steps are needed), **Execute** (delegate to agents in the right order), **Review & Repair** (check outputs, run tests, fix issues), **Finalize** (all tasks done)[\[40\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Workflow%20for%20Implementation%20,agent)[\[34\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,a%20corresponding%20test%20in%20tests). This creates a procedural guide for the orchestrator.

* **Project Conventions/Standards** – a list of rules that must be enforced (coding style, project architecture, testing requirements, documentation, etc.)[\[27\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Repo%20Conventions%20,in%20Colab%20and%20local%20dev).

* **Working Style/Tips** – any advice on *how* to work, such as “make incremental changes”, “avoid large refactors unless necessary”, “always update related docs”, etc. In LNF, they included a “Working style” section reminding the agent to read relevant files first, make minimal changes, and run tests and linters[\[41\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Working%20style%3A%20,basic%20lint%2Fformat%20checks%20if%20available).

* **Definition of Done** – a checklist or bullet list of what constitutes a complete, acceptable outcome[\[42\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Definition%20of%20done%3A%20,notebook%20generation%20once%20those%20exist) (e.g. feature implemented, tests passing, documentation updated, no regression). This helps the orchestrator know when it can conclude the task.

By giving the orchestrator such a clear blueprint, you enable it to manage a complex multi-step process reliably.

**Example (Excerpt):** The excerpt below (simplified from an example orchestrator profile) demonstrates some of these elements:

You are the lead AI agent (Orchestrator) for \*\*Project X\*\*. You coordinate all development phases, delegate tasks to specialists, and ensure the final output meets all project standards.

\#\# Responsibilities  
\- \*\*Orchestration:\*\* Break down user requests into smaller tasks and assign each to the appropriate specialist agent.  
\- \*\*Standard Enforcement:\*\* Ensure all code and artifacts follow Project X's conventions (coding style, directory structure, testing requirements).  
\- \*\*Verification:\*\* Review outputs from subagents and run tests to verify everything works together.

\#\# Delegation Rules (IMPORTANT)  
\- Always use the \`agent\` tool to delegate work – \*never\* assume the role of another agent yourself.  
\- Do not attempt to write feature code directly if a specialist agent exists for that domain.

\*\*Agents Map:\*\*  
\- \*\*UI changes\*\* → use \`frontend-agent\` (for any task involving frontend code or interface).  
\- \*\*Database/model changes\*\* → use \`db-agent\`.  
\- \*\*Writing tests\*\* → use \`test-agent\`.  
\- \*\*Documentation\*\* → use \`docs-agent\`.  
\*(If a request spans multiple areas, delegate each part to the respective agent.)\*

\#\# Workflow  
1\. \*\*Analyze Request:\*\* Understand the user’s feature request or bug report. Read relevant context (specs or existing code) if needed.  
2\. \*\*Plan Tasks:\*\* Determine which aspects are involved (UI, backend, etc.) and which agents to invoke. Outline a plan.  
3\. \*\*Execute (Delegate):\*\* For each part of the work, call the appropriate agent. Provide a clear prompt with requirements. For example: \*“/agent frontend-agent Add a new settings panel to the UI with options A, B, C.”\*  
4\. \*\*Review & Integrate:\*\* When an agent finishes, use \`read\` to inspect the changes. If it’s not meeting criteria or has issues, give feedback and possibly call that agent again with revisions.  
5\. \*\*Test:\*\* Once all parts are implemented, run the test suite with \`execute\` (e.g. \`execute npm test\` or \`execute pytest\`). If tests fail, figure out which part failed and delegate fixes.  
6\. \*\*Finalize:\*\* Ensure documentation is updated (if not, delegate to \`docs-agent\`). Confirm all tests pass and standards are met.

\#\# Project X Conventions (Must Enforce)  
\- All Python code must follow PEP8 and include type hints.  
\- All new features require at least one unit test in the \`tests/\` folder.  
\- Frontend React code must pass ESLint checks (run \`npm run lint\`).  
\- Data models should be defined in \`models/\` directory and follow our naming schema.  
\- ...

\#\# Definition of Done  
\- ✅ Code for the requested feature is implemented and integrated.  
\- ✅ All tests (old and new) are passing.  
\- ✅ CI checks (linting, formatting) are clean.  
\- ✅ Documentation and changelog are updated for the new feature.

The above illustrates how an orchestrator is configured to manage and oversee a multi-agent development process.

**Handoff vs Tool Delegation:** It’s worth noting that in some environments (like VS Code Insiders), GitHub introduced a *handoff* mechanism where an orchestrator can suggest a next agent and prompt, allowing the user to click a button to switch context (e.g. “Start Implementation ➡️”)[\[43\]](https://code.visualstudio.com/docs/copilot/customization/custom-agents#:~:text=To%20define%20handoffs%20in%20your,an%20optional%20prompt%20to%20send)[\[44\]](https://code.visualstudio.com/docs/copilot/customization/custom-agents#:~:text=match%20at%20L525%20,shown%20on%20the%20handoff%20button). However, on GitHub.com and the CLI, this interactive handoff isn’t supported in the agent profile (the handoffs property is ignored[\[17\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=Nota%3A)). Therefore, our orchestrator relies on using the agent tool *within the chat flow* to delegate tasks. In VS Code, you also have an experimental runSubagent tool that lets an agent spin up a subagent in isolation[\[45\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=In%20case%20of%20automatic%20orchestration,that%20everything%20works%20as%20expected). The concept is similar – one agent triggering another – but implemented differently. This guide focuses on the standard GitHub workflow, so we assume the orchestrator will delegate by calling subagents via the agent tool in-line (which is what the tools: \["agent", ...\] enables). The result is functionally the same: the orchestrator’s session drives the process, pulling in subagent contributions as needed.

## Specialist Agents (Subagents for Specific Tasks)

Specialist custom agents are the “team members” that the orchestrator calls upon to handle well-defined domains or stages of work. Each specialist is configured to be an expert in one area – for example, writing documentation, generating a Jupyter notebook, creating unit tests, performing security analysis, etc. They follow the single-responsibility principle: **one agent \= one domain or phase of work**. This makes their scope narrow and their instructions focused.

**Characteristics of specialist agents:**

* **Focused Scope:** A specialist’s prompt should make clear exactly what it works on. For instance, *“You implement Phase 4: Notebook Generation & Export”*[\[46\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=You%20implement%20,Generation%20%26%20Export) or *“You are a testing specialist focused on improving code quality through comprehensive testing”*[\[47\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=You%20are%20a%20testing%20specialist,Your%20responsibilities). They typically have knowledge of only their part of the project. It’s helpful to explicitly list the boundaries of their work in a **Scope** section. For example, a “notebook generator” agent’s scope might include which modules or files it should create/modify (as seen in LNF’s notebook agent: it lists specific files like src/notebook/composer.py, templates.py, etc. to build)[\[48\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Scope%3A%20,if%20dependencies%20are%20already%20included). A testing agent’s scope may include writing new test files and possibly updating test configurations, but explicitly not touching application code[\[49\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,production%20code%20unless%20specifically%20requested). By outlining scope, you prevent the agent from straying into areas handled by others.

* **Limited Tools:** To enforce the focused scope, give each specialist only the tools it truly needs. For example, a documentation agent might only need read (to read code or existing docs) and edit (to write docs) – it might not need execute or search if its job is straightforward writing. A testing agent might need edit (to write test files) and execute (to run tests), plus read (to inspect code under test). Meanwhile, it probably shouldn’t have the agent tool (a subagent typically shouldn’t delegate further, unless you have a nested hierarchy). In the YAML frontmatter, list the minimal toolset for each specialist[\[50\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=description%3A%20Creates%20detailed%20implementation%20plans,). This helps avoid the agent performing unintended actions (for instance, a test specialist with no edit tool wouldn’t be able to alter code even if it tried – but you probably *do* want it to edit test files, so allow that, but its prompt will tell it not to edit production code). Remember, if tools is omitted entirely, the agent would have access to everything, which might be too powerful for a subagent and lead to “drift” (LLM going off-task)[\[51\]](https://www.reddit.com/r/GithubCopilot/comments/1ojemfh/custom_agent_handoff_first_impressions/#:~:text=7,tools%2C%20and%20too%20many%20instructions). So it’s recommended to explicitly specify tools for specialists as a form of sandboxing. (See **Scoped Tool Usage** below for more on choosing tools.)

* **Domain-Specific Instructions:** The prompt content of a specialist agent should contain all the domain knowledge, guidelines, and steps relevant to its niche. Structure it in a way that’s easy to follow. Many specialists benefit from sections like:

* **Scope** – What this agent is responsible for (and possibly by omission, what it isn’t). Example: *“Scope: Build the notebook composer and exporter modules; convert high-level spec objects to .ipynb format; provide export options (ipynb, zip, PDF)…”*[\[48\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Scope%3A%20,if%20dependencies%20are%20already%20included).

* **Constraints** – Any limitations or rules specific to this domain. Example: *“Constraints: The generated notebooks must run in Google Colab; include specific standardized cells; avoid any network calls at runtime unless the user specifically asks.”*[\[52\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Constraints%3A%20,requested%20by%20the%20user%20prompt). Another example: a security audit agent might have “Constraints: Only read the code, do not modify it; flag any risky patterns; do not actually fix code (just report).” Writing out constraints helps the agent self-check its behavior.

* **Tasks/Responsibilities** – A list of what the agent should do, phrased as actions. For example, a test agent might list: “- Analyze existing tests to find gaps, \- Write new unit tests for the new feature, \- Ensure tests are isolated and deterministic, \- Avoid modifying production code”[\[53\]\[49\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,production%20code%20unless%20specifically%20requested). A documentation agent might list sections to include in docs, etc. If the agent’s job is more procedural (e.g., a multi-step transformation), you could enumerate steps it should follow.

* **Quality Gates / Verification** – Criteria the agent’s output must meet to be accepted. E.g., “Quality gates: The notebook must validate with nbformat and execute without errors in a smoke test”[\[29\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Quality%20gates%3A%20,where%20feasible), or “All new tests should pass and increase coverage by X%,” or “Generated code should compile without syntax errors,” etc. Essentially, how to know the subagent did a good job. This often overlaps with what the orchestrator will check, but putting it in the subagent’s mind can improve its output on the first try. For instance, the notebook agent knows upfront that it should validate the notebook format and possibly run a test execution, so it will aim to produce a notebook that meets those criteria[\[29\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Quality%20gates%3A%20,where%20feasible).

* **Style/Standards** – If there are conventions specific to the domain, list them. For example, for a docs agent: “Use our markdown style guide: level-2 headings for sections, capitalize titles, use American English.” For a code agent: “Follow the project’s lint rules (no unused vars, 2-space indentation, etc.).” These could be included in a “Constraints” or their own section.

* **No Cross-Over**: Reinforce that the specialist should stick to its domain and not handle tasks outside it. The test from GitHub’s example prompt says *“Focus only on test files and avoid modifying production code unless specifically requested.”*[\[49\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,production%20code%20unless%20specifically%20requested) – this is a perfect illustration of restraining the agent. If the orchestrator accidentally asks a subagent to do something slightly outside its scope, the subagent (if following instructions) might either refuse or at least do minimal changes. This division of labor ensures each agent’s output is easier to trust and verify.

**Example structure for a specialist agent prompt:**

Imagine a **“Test Specialist”** agent that writes tests. Its file might contain:

\---  
name: test-specialist  
description: Writes thorough tests for new features without altering production code.  
target: github-copilot  
infer: false  
tools: \["read", "edit", "execute"\]  \# can read code, write test files, and run tests  
metadata:  
  project: "MyProject"  
  role: "test"  
\---

You are a \*\*Testing Specialist\*\* AI focused on improving code quality through tests.

\*\*Scope:\*\*    
\- Create new unit tests and integration tests for features as directed.    
\- Only modify or create files under the \`tests/\` directory (or clearly designated test files).    
\- Do \*\*not\*\* alter source/application code unless explicitly asked to (testing is your primary job).

\*\*Responsibilities & Tasks:\*\*    
\- Analyze the existing test suite to identify areas relevant to the feature or component under test.    
\- Write new test cases covering expected behaviors, edge cases, and error conditions.    
\- Use the project’s testing framework (e.g. PyTest, JUnit, etc.) and follow its conventions.    
\- Ensure tests are isolated (no external network calls, use mocks/fakes as needed), deterministic in outcome, and well-documented (clear names and comments).

\*\*Constraints:\*\*    
\- \*\*No production code changes:\*\* Focus only on creating or updating tests. If a production code change is required (e.g., to enable testing), flag it rather than making it yourself (unless instructed).    
\- Adhere to the project’s style guidelines for code and test structure. For example, if the project uses AAA (Arrange-Act-Assert) pattern in tests, follow that.

\*\*Quality Gates:\*\*    
\- All new tests should \*\*pass\*\* (obviously) and not break existing tests.    
\- Tests should meaningfully increase coverage for the target feature or module.    
\- If relevant, run the test suite (\`execute\` tool) to verify nothing fails after adding your tests.    
\- The test code should be clean (will be subject to code review or linters).

Always include descriptive names for test functions and concise assertions. Aim for clarity and maintainability in the test code. If you encounter unclear behavior, you may also document it or prompt for clarification rather than guessing.

This example demonstrates a structured approach with labeled sections (Scope, Responsibilities, Constraints, Quality). It reads almost like an onboarding note to a human tester, which is a good way to think about it – you’re instilling all the domain knowledge and rules that a competent specialist in that area would know.

**Another example:** the earlier provided lnf-notebook.agent.md (Notebook Generator agent) was structured as:

* A sentence stating its phase responsibility (*“You implement Phase 4: Notebook Generation & Export.”*)[\[54\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=You%20implement%20,Generation%20%26%20Export).

* A **Scope** list of specific files to create and functionalities to implement (converting CellSpec to .ipynb, providing exporters, etc.)[\[48\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Scope%3A%20,if%20dependencies%20are%20already%20included).

* **Constraints** bullet points (notebooks must run in Colab, include specific cells as per plan, avoid network calls)[\[52\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Constraints%3A%20,requested%20by%20the%20user%20prompt).

* **Quality gates** (notebook should validate with nbformat, and a smoke test should run key cells)[\[29\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Quality%20gates%3A%20,where%20feasible).

This is a great template for any agent responsible for producing a complex artifact: define the scope, outline constraints (runtime environment, required structure), and quality checks.

**Tool configuration for specialists:** As mentioned, keep it minimal. If the agent only needs to read and write files, don’t give it web or search unless necessary. If it might need to look up something in the repo, search is useful. If it should run tests or code, execute is needed. Do allow edit for any agent that is expected to actually create or modify files (most will, except maybe a pure analysis agent). Many specialist agents won’t need the agent tool – they are the end of the line in delegation. Giving a specialist the ability to call another agent could complicate your workflow unless you intentionally design a hierarchy (which is rare; usually one orchestrator → multiple specialists is enough). Also consider if the specialist needs any MCP tools (probably not unless using some API via MCP) – usually not in a straightforward code generation scenario.

In the GitHub docs example, they showed an **Implementation Planner** agent that explicitly only had tools: \["read", "search", "edit"\] (omitting execute, etc.) since its job was just to produce a markdown plan[\[55\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,). Tailor the tools to the role in a similar way.

## Planning Your Agent Architecture: Phases vs. Specialties

When setting up multiple custom agents, you should decide how to split responsibilities among them. Two common strategies (which can be combined) are **phase-based workflows** and **task/specialty-based segmentation**:

* **Phase-Based Workflow:** Agents are organized by stages of a development process, to be run sequentially. For example, you might have an **Analysis** agent (Phase 1), a **Planning** agent (Phase 2), an **Implementation** agent (Phase 3), a **Testing** agent (Phase 4), and a **Documentation** agent (Phase 5), each corresponding to a step in a pipeline. An orchestrator (or a human) could invoke these in order to carry a feature from conception to completion. The EPAM guide on multi-agent workflows illustrates this with agents named 01\_analyze\_requirements, 02\_create\_plan, 03\_implement, 04\_test\_validate, 05\_document, plus a top-level orchestrator 00\_orchestrator that runs the sequence[\[56\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=github,Testing%20agent)[\[57\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=name%3A%20). In that model, each agent knows it’s part of a numbered step and focuses strictly on that phase’s output, handing off to the next. Phase-based agents often rely on outputs from previous phases (e.g., the Testing agent reads the results of Implementation). You can encode these dependencies in their prompts (e.g., the Testing agent prompt might say “Use the implementation results from Phase 3 (file X) as the basis for testing” – possibly facilitated by the orchestrator passing that context). This approach works well for **long-running, structured tasks or project planning** scenarios.

* **Task/Specialty-Based Segmentation:** Agents are split by concern or skill area, and the orchestrator dynamically picks which ones to use based on the request. This is the case in the **LNF example**, where agents are divided by technical function: one for RAG (retrieval-augmented generation) tasks, one for graph logic, one for notebook export, one for QA/testing, one for docs, etc.[\[25\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,). There isn’t necessarily a fixed sequence; the orchestrator might invoke only a subset of these agents depending on what the user asks. This is akin to having a team of specialists (designer, coder, tester, writer) ready to respond when needed. It works well for **feature development or issue-solving** where each request might touch different areas of the codebase and you need domain experts for each part.

These two approaches aren’t mutually exclusive. You could have a hybrid: for example, an orchestrator that first uses a “Planner” agent to outline work (phase-based step), then uses a set of specialists to implement various parts in parallel (task-based). Or you might primarily use task-specialists, but have a separate “Audit” agent that runs after everything for QA (phase of final review).

**Design your architecture based on your project’s workflow:** If your development process is very sequential (plan → code → test → deploy), phase agents can mirror that. If it’s more about handling different concerns (frontend vs backend, or code vs docs vs tests), role-based specialists make sense. You can also organize agents around team roles (e.g., “DevOps agent” for CI configs, “Security agent” to review code for vulnerabilities, etc.).

When integrating a new agent into an existing multi-agent setup, consider where it fits in this structure. Does it handle a new phase (e.g., a new Step 6 for Deployment)? Or is it a new specialty parallel to others (e.g., a new AI Code Reviewer agent that the orchestrator could call when needed)? Answering that will guide how you write its prompt and how you update the orchestrator.

If you use **phase numbering or naming conventions**, reflect that in the file names for clarity. For instance, prefixing file names with 01\_, 02\_ can help humans and orchestrators (if you programmatically list agents) know the order. In the directory listing below, you can see an example structure with numbered phases and an orchestrator coordinating them:

[\[56\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=github,Testing%20agent)

Each file corresponds to a step in the workflow, and the orchestrator is step 0\. Alternatively, if using specialty agents, you might prefix with the project or domain (like proj-docs.md, proj-test.md) or simply use the role (docs.agent.md, test.agent.md). The naming is flexible, but consistency helps avoid confusion. Also, try to avoid naming collisions with existing or future agent names (since if two agents have the same name in YAML at different levels, the repo one overrides the org one, etc.[\[58\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=Custom%20agents%20names)).

**Tip:** Document your architecture in your repo’s README or a AGENTS.md file so that contributors understand what each agent is for and how they interact. The agent profiles themselves should be clear from their content, but an overview diagram or table in documentation can be invaluable for onboarding new team members to this multi-agent system.

## Scoped Tool Usage and Permissions

One of the powerful features of custom agents is the ability to **restrict their tool access**. By limiting tools, you not only enforce the agent’s scope but also reduce the chance of it performing unintended actions. Here are some best practices for tool configuration:

* **Principle of Least Privilege:** Give an agent only the tools it needs to accomplish its role, and nothing more. If an agent purely generates documentation, it likely doesn’t need to execute shell commands. If an agent’s job is analysis (read/search) without making changes, you might omit edit/write tools (making it read-only). For example, a read-only planning agent might have tools: \["read", "search"\] and nothing else. (The GitHub Docs example implementation-planner agent does exactly that, enabling only read/search/edit and excluding execute since it writes plans, not code[\[55\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,).) This containment can prevent, say, a planning agent from accidentally editing files or a test agent from trying to push a git commit (there is a github tool set for PRs, but you’d typically reserve that for an orchestrator if at all).

* **Include essential tools for the task:** Conversely, make sure not to hamstring the agent by forgetting a needed tool. For instance, a code-generating agent must have the edit (or more specific write) tool to actually make changes to files[\[59\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=Primary%20alias%20Compatible%20aliases%20Coding,Exact%20arguments%20can%20vary). If it should create new files, the edit tool covers that (in Copilot, the edit tool can usually handle creating new files when given a path). If the agent needs to search the codebase for references (say, a security agent looking for usage of a function), give it the search tool so it can grep or use the code index[\[59\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=Primary%20alias%20Compatible%20aliases%20Coding,Exact%20arguments%20can%20vary). If it should run code or tests, it needs execute (which maps to running shell commands)[\[59\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=Primary%20alias%20Compatible%20aliases%20Coding,Exact%20arguments%20can%20vary).

* **Use wildcards for convenience:** Copilot defines some tool aliases and wildcard syntaxes for groups of tools. For example, github/\* enables all the read-only GitHub API tools (like reading issues, discussions, etc.)[\[60\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=MCP%20server%20name%20Available%20tools,to%20is%20scoped%20to%20the), and playwright/\* would enable all browser automation tools (useful if your agent needs to open pages or screenshots, etc.)[\[61\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=%60github%20%60All%20read,to%20the%20source%20code%20repository). In the LNF lead agent, they included both of these: "github/\*", "playwright/\*" in tools[\[62\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=tools%3A%20%5B,), likely to allow the orchestrator to fetch repository content or take screenshots of notebooks. Consider what external or extended tools your agent might need. If none, you can ignore wildcards. (By default, the GitHub MCP provides github and playwright servers for codebase read-only access and browser tasks, respectively[\[63\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=The%20following%20MCP%20servers%20are,can%20be%20referenced%20using%20namespacing).)

* **Preventing dangerous actions:** If you are concerned about certain tools, you can simply not list them (e.g. omit execute if you never want that agent running commands). The docs also mention you can explicitly disable all tools with tools: \[\][\[64\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=%60azure.some,all%20tools%20for%20the%20agent), but more commonly you’d just not include a tool rather than include and deny it. There isn’t a granular permission system in the YAML for repository-level agents (unlike the gist we saw for another system with permission: deny entries[\[65\]](https://gist.github.com/gc-victor/1d3eeb46ddfda5257c08744972e0fc4c#:~:text=permission%3A%20edit%3A%20deny%20bash%3A%20,deny%20webfetch%3A%20deny) – that was a different setup). So basically, *whatever isn’t listed is off-limits*. This implicitly sandboxes the agent. For instance, if an agent doesn’t have web in tools, any attempt it makes to call an internet search or fetch will be ignored. This helps keep the agent focused and also addresses security (the agent can’t make network calls unless allowed, etc.).

* **Agent Tool (Delegation) usage:** As covered, only orchestrator-type agents should have the agent tool enabled in most cases. If you give a subagent the agent tool, it could in theory start invoking other agents, which complicates your workflow and could lead to circular calls or confusion. Unless you have a very advanced scenario (like a subagent that itself delegates a sub-task further), it’s safer to not give subagents the capability to spawn others. Keep the hierarchy shallow: orchestrator at the top uses agent tool, specialists do their job and return results.

* **Testing and adjusting tools:** After configuring, keep an eye on the agent’s behavior. If it ever replies saying “I don’t have permission to do X” or fails to complete a task because a tool is missing, you might need to add that tool. Conversely, if an agent is doing things outside its role, check if you inadvertently left it too much power. For example, if a doc agent somehow modified code, maybe it had the edit tool which allowed it – you might refine its prompt to explicitly forbid that (as we did) or even consider splitting the edit tool into more granular ones if available (the Copilot environment might not allow per-directory restrictions, so mostly it’s by convention).

In summary, **the tools field is your friend in shaping what an agent can and cannot do**. Pair the technical restriction (not listing a tool) with prompt instructions (telling the agent not to do something) for best results. The agent will usually follow its instructions, but the tool limits are a crucial safety net that guarantees compliance.

## Step-by-Step: Adding a New Custom Agent to Your Architecture

When your project grows or your needs change, you might decide to add another custom agent to the mix. Here’s a step-by-step process to design and integrate a new agent effectively:

1. **Identify the Gap or Role for the Agent:** Determine what new responsibility or workflow the agent will cover. Is there a repetitive task or a specialized area (e.g., performance tuning, UI design, API documentation) that isn’t well-handled by your existing agents? The new agent should have a clear purpose. For example, you realize you need a **“Security Audit Agent”** to scan code for vulnerabilities, which your current setup lacks.

2. **Choose a Name and File Name:** Pick a unique name (lowercase with hyphens or underscores, no spaces) for the agent. This will go in the name field and the filename. The name should reflect its role (e.g., security-auditor, performance-tuner, data-migrator). Consistency with your naming convention is key – if all your agents are prefixed or numbered, follow that. For instance, if you use phase numbers, maybe this is 06\_security.auditor.md for a new step 6; if by specialty, maybe myproj-security.agent.md. Create a new Markdown file under .github/agents/ with this name.

3. **Draft the YAML Frontmatter:** Start the file with the \--- YAML header. Fill in the name: (the unique ID from step 2\) and a description:. The description should succinctly explain the agent’s role for anyone browsing agents (and appears in the chat UI placeholder text)[\[3\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=Metadata%20Field%20Description%20,the%20filename%20if%20not%20specified). For example: description: Reviews code for security issues and suggests fixes. Next, decide on tools: – list only those required. A security audit agent might need read (to read code), maybe search (to find patterns), possibly web if it should fetch known vulnerability info, and edit if you want it to suggest fixes in code (though it could also just comment rather than directly fix). If it’s only analyzing, perhaps you exclude edit. Set infer: false usually, unless you want Copilot to auto-pick it (generally false to avoid surprise activations). Add any metadata that helps (e.g., role: "security" or phase: "6"). The frontmatter might look like:

* \---  
  name: security-auditor  
  description: Identifies security vulnerabilities in the codebase and suggests remediation  
  target: github-copilot  
  infer: false  
  tools: \["read", "search", "edit"\]  
  metadata:  
    project: "MyProject"  
    role: "security"  
  \---

* *(Omit edit if you* only *want it to report issues and not make changes itself.)*

4. **Write the Agent’s Prompt (Markdown Body):** Now spend time crafting the instructions. Start with a clear **role declaration** (e.g., “You are a **Security Auditor** agent. Your job is to review code for security issues, not to implement features.”). Define its **scope** – does it scan the entire repository? Only certain modules? Does it handle dependency checks? Also clarify what it should *not* do (e.g., not actually fix code unless asked, not run any destructive operations). Then list its **responsibilities** or tasks in bullets: e.g., “- Scan code for OWASP Top 10 vulnerabilities, \- Check for secrets or API keys committed, \- Review dependency versions for known vulnerabilities, \- Suggest improvements or flag dangerous code.” Include any **constraints**: e.g., “Do not modify code directly unless instructed; focus on analysis and suggestions. Do not raise purely stylistic issues – only security-relevant ones.” If relevant, mention any *tools usage* like “Use the search tool to find occurrences of certain patterns (e.g., exec( usage).” Add **quality/format guidelines**: e.g., “Output a report in markdown format with sections for each issue.” Essentially, encode how you’d like its answer to look. By making this explicit, the agent will format its responses helpfully. Keep paragraphs short and use subheadings if the content is long (for readability). Remember to leverage your knowledge of the domain – if certain vulnerabilities are priority (SQL injection, XSS, etc.), mention them so the agent knows to look. This becomes the “playbook” the agent will follow.

5. **Review and Align with Existing Agents:** Before finalizing, consider if this new agent overlaps with any existing one. If so, adjust scope to reduce conflict. (For instance, if you had a general “QA agent” that did some security checking, you might now refocus it on other QA aspects and let the security agent handle security exclusively.) Ensure the descriptions and metadata distinguish them clearly.

6. **Update the Orchestrator (if applicable):** This is crucial in a multi-agent architecture. Go to your orchestrator agent’s profile and incorporate the new agent into its worldview. Add the new agent to its **capability map** or delegation logic. For example, extend a mapping table or list: *“Security Review: call security-auditor agent to scan the code for vulnerabilities.”*[\[66\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=%7C%20,). If your orchestrator’s workflow should include a security phase, add a step in its **Workflow** section. E.g., after implementation and testing, a step: “**Security Check** – Invoke the security-auditor agent to review the changes for vulnerabilities; address any findings.” Also, in the orchestrator’s **tools** list, ensure it has whatever tools needed to invoke or process results (the orchestrator might just use agent to call it and then read its report). Essentially, teach the orchestrator when and how to use the new agent. This could also involve adding a **handoff** suggestion if you were using those in VS Code (but on GitHub, just the mapping and workflow text suffice). By updating the orchestrator, you fully integrate the new agent into the team, so it won’t be forgotten.

7. **Place and Commit the Agent File:** Save the new agent markdown file in .github/agents/. If you have multiple agents, you might keep them sorted by name or number for organization. Commit the file to your repository (likely on the default branch or whichever branch you’re testing on). Remember that the custom agent profile is versioned by git SHA – Copilot will use the latest committed version on the branch you’re working in[\[58\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=Custom%20agents%20names). So if you’re testing on a feature branch, ensure it has the updated agent file.

8. **Test the New Agent in Isolation:** It’s time to try it out. Open a Copilot chat (for example, using the GitHub CLI gh copilot chat, or on GitHub.com via the Codespace or Repo UI if available, or VS Code if you have the repo open there). Manually select the new agent from the agents dropdown (since infer: false, it won’t auto trigger). Then give it a typical prompt it should handle. For our security auditor example, you might say: “Find any security issues in the authentication module.” See how it responds. Does it follow the instructions? Does it format the output as expected? Is it using only allowed tools (you might see tool usage in the conversation, or check logs if available)? If the agent tries to do something it shouldn’t or gives an irrelevant answer, this is your chance to refine its prompt. You might discover you need to add an instruction or clarify a point.

9. **Test Integration via Orchestrator:** Now test the whole system. For instance, ask the orchestrator for something that should involve the new agent. Perhaps: “Add a login feature and ensure it’s secure.” The orchestrator should delegate the coding to appropriate agents and then call the security-auditor as part of the process. Watch the interaction: does the orchestrator indeed invoke the new agent at the right time? Does the new agent provide useful output back? This end-to-end dry run is critical for catching any coordination issues. If the orchestrator didn’t call it when expected, maybe you need to tweak the orchestrator’s prompt or ensure the trigger words match (the orchestrator might not have recognized it needed a security check – maybe update the workflow to always do it, or if it’s conditional, ensure the condition triggers).

10. **Iterate on Instructions:** It’s common that the first draft of an agent profile might need adjustments once you see real outputs. Don’t hesitate to iterate. Update the Markdown prompt with additional bullet points or rephrased instructions if you see misunderstandings. For example, if the security agent reported too verbosely, you might add “Provide a concise summary for each issue.” If it missed certain vulnerabilities, emphasize those in the instructions. Each commit and re-run will use the new profile version, so you can refine fairly quickly. Just be mindful of the 30k character limit – but that’s quite large, so most likely you won’t hit it unless you paste huge reference materials into the prompt (which you generally shouldn’t; instead have the agent use read to fetch files as needed).

11. **Team Review (if applicable):** If you’re in a team setting, have another developer or stakeholder review the new agent’s config file. It’s essentially part of your codebase/documentation. Make sure the description is clear, and the guidelines align with team expectations. Since these agents can significantly influence code, treating their definitions with the same rigor as code reviews is wise. A colleague might spot an instruction that’s ambiguous or suggest an additional rule to add.

12. **Maintain and Evolve:** Once integrated, monitor how the agent performs in practice. If you notice in future uses that it occasionally goes off track, update its prompt. If your project’s conventions change (say you adopt a new testing framework), update the relevant agents’ instructions to reflect that. Custom agents are meant to be living documentation of your processes; keep them up-to-date as the single source of truth for your AI helper. The beauty is, when you improve an agent profile, everyone using it (you or teammates) immediately benefits from the refined behavior.

Following these steps ensures a smooth addition of a new agent to your multi-agent architecture, with minimal disruption and clear responsibilities.

## Repository Layout, Naming Conventions, and Management

Organizing your .github/agents directory thoughtfully will pay off as your number of agents grows:

* **File Naming:** Use consistent naming for agent files. A good pattern is \<project\>-\<role\>.agent.md or \<number\>\_\<role\>.md. For example, the LNF project used names like lnf-lead.agent.md and lnf-notebook.agent.md to clearly identify the project (“LNF”) and role[\[62\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=tools%3A%20%5B,)[\[16\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=metadata%3A%20project%3A%20,4). If all your agents are for one project, the project prefix might be redundant, so you could just do lead.agent.md, notebook.agent.md, test.agent.md, etc. Alternatively, the numbered approach (00\_orchestrator.md, 01\_analysis.md, 02\_plan.md, ...) as shown in an earlier example[\[56\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=github,Testing%20agent) helps when there’s a natural order. Choose what makes sense and stick to it. Remember, the name: inside the file can be the same as the file name (minus extension) for simplicity. Avoid spaces or uppercase in file names to prevent any potential issues or confusion.

* **Directory Structure:** All repository-level custom agents should reside directly under .github/agents/. Subfolders within it are generally not used for agent profiles (the system likely won’t detect agents nested in subdirectories). So, keep all agent .md files in that one folder. If you have dozens of agents, naming becomes even more important to visually group them (perhaps by prefix or numbering as mentioned). You might also add a short README in that folder listing the agents and their purpose, for maintainers’ sake (optional, since one can always open each file).

* **Source Control and Versioning:** Each agent profile is versioned by Git commits. This means if you update an agent and commit, anyone who pulls that commit and uses Copilot Chat will use the new behavior. If you have long-running branches, be mindful that the agent’s behavior on that branch is defined by whatever version of the file is there. In case you need to maintain different versions (perhaps an older release branch with slightly different processes), this versioning by branch can be beneficial. Just note that if the agent creates a Pull Request, the interactions in the PR will use the version from that branch commit point for consistency[\[67\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=Custom%20agents%20names).

* **Testing Changes:** It’s wise to test agent profile changes in a safe environment (like a non-production branch or fork) to avoid the AI doing something unexpected in a live setting. Because these agents can be assigned to issues and PRs, an incorrect instruction could lead to unwanted changes. Use dry runs as described to validate behavior.

* **Naming in UI:** The name field of the agent profile is what shows up in Copilot’s UI (the chat dropdown or CLI list). If you omit name in YAML, Copilot will derive it from the file name[\[3\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=Metadata%20Field%20Description%20,the%20filename%20if%20not%20specified). It might also prettify it (e.g., a file security-auditor.md with no name might show up as “security-auditor” or maybe “Security Auditor” if it guesses spacing). It’s usually better to explicitly set a friendly name in YAML, especially if your file name is abbreviated. For instance, name: Security Auditor (though be cautious using spaces; test how it appears – documentation indicates name should be a unique identifier, which usually means no spaces, but in VS Code custom chat modes they did allow a display name with spaces. On GitHub, to be safe, you might keep it without spaces or just use the file name).

* **Agent Discovery:** In GitHub’s interface, users can choose an agent for an issue or chat. At repository level, they will only see those agents defined in that repo’s .github/agents/. At org or enterprise level (if using .github-private), they see those too. Keep in mind that if an agent is meant only for orchestrator’s internal use and not for direct user queries, you still have to define it in the repo (there’s no hidden agents concept). But you might not advertise it. For example, maybe you have a “Refactoring agent” that orchestrator calls; you could still manually pick it, but generally you’d just let orchestrator use it. That’s fine – just note all agents in the repo are technically selectable by users with access.

* **Conventions:** If certain patterns emerge (like every agent has a metadata.role), you might create a template or snippet for new agents. Possibly maintain a **contributing guide** for your team on how to add/modify agents. Since this is a new frontier, having shared guidelines (like this document\!) can ensure consistency.

* **Removing or Renaming Agents:** If you ever need to deprecate an agent, be mindful of any references to it (especially in orchestrator prompts or handoffs). Remove or update those references. Renaming an agent file means its name (if not set or if matching filename) changes, which could confuse older issues or PRs where it was used. It’s akin to changing an API – not a huge deal, but track it in your change logs.

* **Security and Secrets:** Do not put sensitive info (API keys, etc.) in agent profiles. If the agent needs a token to call an API via MCP, use environment variables configured in the repo (Copilot supports referencing those in MCP config, as seen by the ${{ secrets.VAR }} syntax for MCP server secrets[\[68\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=MCP%20server%20environment%20variables%20and,secrets)[\[69\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=patterns%3A)). The agent profiles are part of your code repository – treat them as public (even if your repo is private) from the AI’s perspective. They should contain instructions and rules, not actual credentials. Also be cautious about any proprietary info in the prompts; the agents will likely be run in contexts that could log or expose their instructions (e.g., if someone not on your team triggers it, they might see parts of the agent’s system message if the AI explains its reasoning). Generally, keep the content high-level and policy-oriented, not secret.

In short, manage the .github/agents directory like a config directory for your AI assistants. Clean, well-named, and documented agent profiles will make it easier to scale your multi-agent setup.

## Testing and Refining Your Agents

Writing the agent configuration is only the first step – ensuring it works as intended is equally important. Here are strategies for testing and iterating on custom agents:

* **Manual Chat Testing:** The simplest way to test an agent is to engage with it directly in a chat environment. For repository-level agents, you can do this on GitHub.com by opening the Copilot chat (if available) or using the GitHub Copilot CLI (gh copilot chat). Select your custom agent from the agents dropdown or via the CLI command. Then simulate a user query that the agent should handle. For example, if you have a **Docs Agent**, ask it: “Create initial README for the project.” If it’s a **Backend Agent**, maybe give it a prompt like: “Implement a new API endpoint for user login.” Observe:

* Did the agent respond with an action (like creating files) or with a helpful answer? (Depending on the interface, you might see it using tools to actually make changes, or it might just describe what it would do; in Copilot CLI, tool usage is actually executed with your approval, etc.)

* Does the content of its answer align with the instructions? (e.g., did the docs agent structure the README properly? Did the backend agent only touch backend files?)

* Are there any signs of confusion or the agent going out of scope? If yes, you may need to tighten its prompt.

* If the agent tries to use a tool it doesn’t have (you might see an error or it saying “I cannot do X”), that’s a hint you should add that tool or adjust the prompt so it doesn’t attempt it.

Testing in isolation helps verify that the agent **understands its own profile**.

* **End-to-End Workflow Testing:** If you have an orchestrator and specialists, test typical workflows with the orchestrator coordinating. For instance, create a dummy GitHub Issue (or just imagine one) that requests a new feature covering multiple areas. Assign it to Copilot with your orchestrator agent (or just prompt the orchestrator in chat with the request text). Watch the sequence of actions:

* Does the orchestrator correctly identify sub-tasks and invoke the right subagents? (Check the agent’s output or any logs – it might say “Invoking X agent for Y task.” If it doesn’t, and tries to handle something itself, you may need to remind it in the prompt about delegating that part.)

* Do the subagents produce good results? (If a subagent failed or produced errors, see why. Perhaps the prompt for that agent or orchestrator’s instructions to it need improvement.)

* Does the orchestrator catch issues and loop back as expected? For example, if a subagent’s output missed a test, did the orchestrator notice and ask for it? You can test by intentionally creating a scenario: maybe remove a test and see if orchestrator insists on it because of the “every feature must have a test” rule.

* Are all final criteria met? If not – say documentation wasn’t updated – does the orchestrator prompt a docs agent?

This is a bit like integration testing for your AI agents. Some companies even create automated tests for their agents by scripting example conversations and verifying outputs (though that’s advanced). At least running through a few representative scenarios manually is highly recommended.

* **Iterative Refinement:** Treat the agent profiles as living documents. It’s common that after a few real uses, you’ll discover something to tweak. For example, maybe the agent gave too verbose responses; you can add “be concise” in its prompt. Or maybe it didn’t realize it should run tests; you might emphasize that step more. Each refinement can be committed and will improve subsequent uses. Think of it as training by instruction – you’re not adjusting model weights, but you are adjusting the “software” around the model (the prompt and tool permissions).

* **Feedback from Users/Team:** If other developers use these agents (via issues or PR chat, etc.), gather their feedback. Perhaps they’ll note “The planning agent often omits the timeline section.” That’s a cue to adjust the prompt to explicitly mention including a timeline. Or “The codegen agent sometimes uses outdated library usage” – maybe you need to feed it more context or docs (like instruct it to read a certain file for latest patterns). The more real-world usage data you have, the better you can fine-tune the instructions.

* **Leverage Logs/Transcripts:** In some environments (like VS Code’s Copilot panel or possibly CLI with verbose mode), you might see the conversation including the system message (your agent prompt) and the agent’s thoughts. Reviewing these can be insightful. You might realize the agent misunderstood a bullet or there was a conflicting instruction. Resolve any such ambiguities in your next edit of the profile.

* **Testing Edge Cases:** Try unusual or edge-case prompts to see how the agent copes. For example, ask the orchestrator to do something that *should* be impossible or outside scope (e.g., “completely redesign the project architecture”). Does it handle it gracefully (maybe it delegates to multiple agents or asks for clarification)? Or does it produce nonsense? If the latter, you might not fix that entirely (LLMs can hallucinate), but you might decide to add a guideline like “If a request is out of scope, ask for clarification or suggest splitting it up.” Similarly, test the security boundaries – if an agent is not supposed to do something, try prompting it to do that and ensure either the tools forbid it or it refuses because of instructions.

* **Automated Tests (if feasible):** This is an emerging area, but you can consider writing scripts to simulate agent runs. For instance, using the Copilot CLI programmatically: after updating an agent profile, run a known prompt and diff the output to expected output. This can be brittle due to randomness and the nature of LLMs, but limiting temperature or using deterministic modes could make it somewhat stable. At minimum, you could script a run of the orchestrator on a known task and assert that certain files exist or certain phrases appear in outputs. This is advanced and optional, but as teams start relying on agents, some regression testing approach may be valuable.

* **Keep Agents Updated with Code Changes:** If your codebase undergoes major changes (say you refactor file paths or adopt a new architecture), update the agents’ prompts to reflect new realities. For example, if you move from src/ to a different structure, update those references in the profiles so agents know where to put files. If you change coding style (like decide to use Google docstrings instead of reStructuredText), let the documentation agent know. Essentially, treat it as documentation that must be current – stale instructions could lead the AI astray, just like outdated comments would mislead a developer.

One great aspect of custom agents is that improving them creates a positive feedback loop – the more you hone their instructions, the more efficient and reliable they become, saving you effort in the long run. One user noted that by iteratively updating his agents with lessons learned, he encapsulated best practices into “a tidy file” and significantly improved his workflow[\[70\]](https://www.reddit.com/r/GithubCopilot/comments/1ojemfh/custom_agent_handoff_first_impressions/#:~:text=6,file%20with%20more%20detailed%20instructions). This is what you want to achieve: the agent profile becomes a repository of your team’s collective knowledge for that role.

## Conclusion

By following this guide, you can create custom Copilot agents that act as specialized AI teammates, each with a well-defined role in your development process. To recap the key points:

* **YAML Frontmatter**: Configure the agent’s identity (name/description), allowed tools, and context (target, infer flag, metadata) in the header[\[5\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=capabilities%20,true)[\[6\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=%60github,used%20by%20the%20custom%20agent). This sets the boundaries within which the agent operates.

* **Prompt Design**: Write clear and structured Markdown instructions delineating the agent’s persona, scope of work, step-by-step approach, and rules to follow. Use sections, lists, and examples generously[\[30\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Primary%20Responsibilities%20,tests%20to%20confirm%20it%20works)[\[52\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Constraints%3A%20,requested%20by%20the%20user%20prompt). This prompt is effectively the “brain” of the agent – the higher quality it is, the better the agent’s outputs.

* **Orchestrator vs Specialists**: Understand the difference in their roles. Orchestrators manage the flow (delegate, enforce standards, integrate results) and thus require broad tool access and a comprehensive prompt covering project-wide policies[\[39\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,agent%20tool%20to%20assign%20work)[\[71\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=python%20scripts%2Fdemo_retrieval,Constraint%2C%20DocSnippet%2C%20QAReport%2C%20CellSpec%2C%20etc). Specialists focus on one thing (write code in one area, write tests, etc.) with limited tools and targeted prompts[\[53\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,production%20code%20unless%20specifically%20requested)[\[48\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Scope%3A%20,if%20dependencies%20are%20already%20included). Design their profiles accordingly.

* **Workflow Coordination**: Decide if your agents follow a phased pipeline, a task-based invocation, or a mix. Ensure your orchestrator (or your own usage) calls them in the right order, and consider using naming conventions or the handoffs feature (in IDEs) to streamline multi-step workflows[\[43\]](https://code.visualstudio.com/docs/copilot/customization/custom-agents#:~:text=To%20define%20handoffs%20in%20your,an%20optional%20prompt%20to%20send)[\[56\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=github,Testing%20agent).

* **Tooling and Safety**: Use the tools field to restrict capabilities, reducing errors and off-scope actions[\[6\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=%60github,used%20by%20the%20custom%20agent)[\[59\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=Primary%20alias%20Compatible%20aliases%20Coding,Exact%20arguments%20can%20vary). Only grant what’s necessary, e.g. give the agent tool only to agents that need to call others. This keeps each agent operating within its lane.

* **Integration and Testing**: When adding new agents or updating them, integrate changes into the overall system (update orchestrator mappings, etc.) and thoroughly test. Simulate real tasks and inspect the outcomes, refining prompts as needed. The agent profiles are version-controlled, so maintain them like code – iterate and improve continuously.

Custom agents empower you to encode your project’s “rules of engagement” for AI assistance – from coding style guidelines to how to break down a feature implementation. By investing time upfront to write effective agent configs, you enable Copilot to work more autonomously and efficiently on your codebase, following your game plan rather than a generic one.

Keep aligning the agents with your evolving project goals. As you gain experience, you might add more specialized agents or refine orchestrator logic to handle more complex scenarios. Always refer back to official GitHub documentation for new features or changes in custom agents (the field is evolving rapidly), and consider sharing successful agent profiles with the community (there’s an awesome-copilot collection for interesting agents[\[72\]](https://docs.github.com/en/copilot/how-tos/use-copilot-agents/coding-agent/create-custom-agents#:~:text=The%20following%20examples%20demonstrate%20what,copilot%20community%20collection)).

In summary, writing a great custom agent config is about clearly communicating *what the agent should do, how it should do it, and what tools it can use*. Treat the agent as a junior developer who needs very explicit guidance to meet your expectations. With the tips and examples provided, you should be well on your way to creating a fleet of custom Copilot agents that turbocharge your development workflow while staying firmly under your control. Happy orchestrating\!

**Sources:**

* GitHub Docs – *About custom agents*: Overview of custom agent profiles and format[\[73\]](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents#:~:text=Custom%20agents%20are%20specialized%20versions,specific%20practices)[\[2\]](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents#:~:text=Agent%20profiles%20are%20Markdown%20files,their%20simplest%20form%2C%20they%20include)

* GitHub Docs – *Custom agents configuration*: YAML properties (target, tools, infer, metadata, etc.)[\[5\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=capabilities%20,true)[\[6\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=%60github,used%20by%20the%20custom%20agent), tool alias reference[\[59\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=Primary%20alias%20Compatible%20aliases%20Coding,Exact%20arguments%20can%20vary)

* GitHub Docs – *Creating custom agents*: Examples of specialist agent prompts (Testing specialist, Implementation planner)[\[47\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=You%20are%20a%20testing%20specialist,Your%20responsibilities)[\[55\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,)

* Example Agent Profiles: Orchestrator (lnf-lead.agent.md) and Specialist (lnf-notebook.agent.md) structure and content for delegation, workflow, scope, constraints, etc.[\[74\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Primary%20Responsibilities%20,How%20to%20Delegate%20%28CRITICAL)[\[52\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Constraints%3A%20,requested%20by%20the%20user%20prompt)

* EPAM Blog – *Single-Responsibility Agents vs Multi-Agent Workflows*: Multi-step agent orchestration example and directory layout[\[56\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=github,Testing%20agent)[\[57\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=name%3A%20)

* Reddit Feedback on Custom Agents: Importance of limiting tools and capturing best practices in agent profiles[\[75\]](https://www.reddit.com/r/GithubCopilot/comments/1ojemfh/custom_agent_handoff_first_impressions/#:~:text=This%20is%20a%20huge%20deal,encapsulated%20in%20a%20tidy%20file)[\[76\]](https://www.reddit.com/r/GithubCopilot/comments/1ojemfh/custom_agent_handoff_first_impressions/#:~:text=Now%20I%20can%20build%20it,into%20my%20modular%20agent%20files)

---

[\[1\]](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents#:~:text=Agent%20profile%20format) [\[2\]](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents#:~:text=Agent%20profiles%20are%20Markdown%20files,their%20simplest%20form%2C%20they%20include) [\[18\]](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents#:~:text=Where%20you%20can%20configure%20custom,agents) [\[20\]](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents#:~:text=You%20are%20a%20documentation%20specialist,modify%20or%20analyze%20code%20files) [\[73\]](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents#:~:text=Custom%20agents%20are%20specialized%20versions,specific%20practices) About custom agents \- GitHub Docs

[https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-custom-agents)

[\[3\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=Metadata%20Field%20Description%20,the%20filename%20if%20not%20specified) [\[45\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=In%20case%20of%20automatic%20orchestration,that%20everything%20works%20as%20expected) [\[56\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=github,Testing%20agent) [\[57\]](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows#:~:text=name%3A%20) Single-Responsibility Agents vs Multi-Agent Workflows: A Practical Guide for AI Coding Tools

[https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows](https://www.epam.com/insights/ai/blogs/single-responsibility-agents-and-multi-agent-workflows)

[\[4\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,without%20modifying%20production%20code) [\[11\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=The%20,depends%20on%20what%20you%20specify) [\[23\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,production%20code%20unless%20specifically%20requested) [\[47\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=You%20are%20a%20testing%20specialist,Your%20responsibilities) [\[49\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,production%20code%20unless%20specifically%20requested) [\[50\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=description%3A%20Creates%20detailed%20implementation%20plans,) [\[53\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,production%20code%20unless%20specifically%20requested) [\[55\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=,) [\[58\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=Custom%20agents%20names) [\[67\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=Custom%20agents%20names) [\[68\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=MCP%20server%20environment%20variables%20and,secrets) [\[69\]](https://docs.github.com/en/copilot/reference/custom-agents-configuration#:~:text=patterns%3A) Custom agents configuration \- GitHub Docs

[https://docs.github.com/en/copilot/reference/custom-agents-configuration](https://docs.github.com/en/copilot/reference/custom-agents-configuration)

[\[5\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=capabilities%20,true) [\[6\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=%60github,used%20by%20the%20custom%20agent) [\[7\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=,prefixed%20with%20the%20server%20name) [\[8\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=available%20tool%20aliases%2C%20see%20Tool,tool) [\[9\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=The%20following%20MCP%20servers%20are,can%20be%20referenced%20using%20namespacing) [\[10\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=All%20unrecognized%20tool%20names%20are,agent%20profile%20without%20causing%20problems) [\[12\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=Supports%20both%20a%20comma%20separated,true) [\[13\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=,should%20be%20used%20by%20the) [\[14\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=custom%20agent.%20,the%20agent%20with%20useful%20data) [\[17\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=Nota%3A) [\[19\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=annotation%20of%20the%20agent%20with,useful%20data) [\[32\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=,and%20performing%20a%20web%20search) [\[59\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=Primary%20alias%20Compatible%20aliases%20Coding,Exact%20arguments%20can%20vary) [\[60\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=MCP%20server%20name%20Available%20tools,to%20is%20scoped%20to%20the) [\[61\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=%60github%20%60All%20read,to%20the%20source%20code%20repository) [\[63\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=The%20following%20MCP%20servers%20are,can%20be%20referenced%20using%20namespacing) [\[64\]](https://docs.github.com/es/copilot/reference/custom-agents-configuration#:~:text=%60azure.some,all%20tools%20for%20the%20agent) Configuración de agentes personalizados \- Documentación de GitHub

[https://docs.github.com/es/copilot/reference/custom-agents-configuration](https://docs.github.com/es/copilot/reference/custom-agents-configuration)

[\[15\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=metadata%3A%20project%3A%20,) [\[21\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Primary%20goals%3A%20,webui) [\[22\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=python%20scripts%2Fdemo_retrieval,in%20Colab%20and%20local%20dev) [\[24\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Example%20Delegation%20Prompt%3A%20,and%20follows%20the%20project%20patterns) [\[25\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,) [\[26\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=How%20to%20Delegate%20%28CRITICAL%29%20,agent%20tool%20to%20assign%20work) [\[27\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Repo%20Conventions%20,in%20Colab%20and%20local%20dev) [\[28\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Working%20style%3A%20,basic%20lint%2Fformat%20checks%20if%20available) [\[30\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Primary%20Responsibilities%20,tests%20to%20confirm%20it%20works) [\[31\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=When%20to%20delegate%3A%20,webui) [\[33\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,py) [\[34\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,a%20corresponding%20test%20in%20tests) [\[35\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,tests%20to%20confirm%20it%20works) [\[36\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,in%20Colab%20and%20local%20dev) [\[38\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,Review%20%26%20Repair) [\[39\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=,agent%20tool%20to%20assign%20work) [\[40\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Workflow%20for%20Implementation%20,agent) [\[41\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Working%20style%3A%20,basic%20lint%2Fformat%20checks%20if%20available) [\[42\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Definition%20of%20done%3A%20,notebook%20generation%20once%20those%20exist) [\[62\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=tools%3A%20%5B,) [\[66\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=%7C%20,) [\[71\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=python%20scripts%2Fdemo_retrieval,Constraint%2C%20DocSnippet%2C%20QAReport%2C%20CellSpec%2C%20etc) [\[74\]](file://file_00000000e664722fb0fff43c79d82c21#:~:text=Primary%20Responsibilities%20,How%20to%20Delegate%20%28CRITICAL) lnf-lead.agent.md

[file://file\_00000000e664722fb0fff43c79d82c21](file://file_00000000e664722fb0fff43c79d82c21)

[\[16\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=metadata%3A%20project%3A%20,4) [\[29\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Quality%20gates%3A%20,where%20feasible) [\[46\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=You%20implement%20,Generation%20%26%20Export) [\[48\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Scope%3A%20,if%20dependencies%20are%20already%20included) [\[52\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=Constraints%3A%20,requested%20by%20the%20user%20prompt) [\[54\]](file://file_0000000032ec71f5a733a3d8691e8c59#:~:text=You%20implement%20,Generation%20%26%20Export) lnf-notebook.agent.md

[file://file\_0000000032ec71f5a733a3d8691e8c59](file://file_0000000032ec71f5a733a3d8691e8c59)

[\[37\]](https://gist.github.com/gc-victor/1d3eeb46ddfda5257c08744972e0fc4c#:~:text=%23%20Delegation%20,TOOL%20task%3A%20true) [\[65\]](https://gist.github.com/gc-victor/1d3eeb46ddfda5257c08744972e0fc4c#:~:text=permission%3A%20edit%3A%20deny%20bash%3A%20,deny%20webfetch%3A%20deny) orchestrator-agent-creation-guide.md · GitHub

[https://gist.github.com/gc-victor/1d3eeb46ddfda5257c08744972e0fc4c](https://gist.github.com/gc-victor/1d3eeb46ddfda5257c08744972e0fc4c)

[\[43\]](https://code.visualstudio.com/docs/copilot/customization/custom-agents#:~:text=To%20define%20handoffs%20in%20your,an%20optional%20prompt%20to%20send) [\[44\]](https://code.visualstudio.com/docs/copilot/customization/custom-agents#:~:text=match%20at%20L525%20,shown%20on%20the%20handoff%20button) Custom agents in VS Code

[https://code.visualstudio.com/docs/copilot/customization/custom-agents](https://code.visualstudio.com/docs/copilot/customization/custom-agents)

[\[51\]](https://www.reddit.com/r/GithubCopilot/comments/1ojemfh/custom_agent_handoff_first_impressions/#:~:text=7,tools%2C%20and%20too%20many%20instructions) [\[70\]](https://www.reddit.com/r/GithubCopilot/comments/1ojemfh/custom_agent_handoff_first_impressions/#:~:text=6,file%20with%20more%20detailed%20instructions) [\[75\]](https://www.reddit.com/r/GithubCopilot/comments/1ojemfh/custom_agent_handoff_first_impressions/#:~:text=This%20is%20a%20huge%20deal,encapsulated%20in%20a%20tidy%20file) [\[76\]](https://www.reddit.com/r/GithubCopilot/comments/1ojemfh/custom_agent_handoff_first_impressions/#:~:text=Now%20I%20can%20build%20it,into%20my%20modular%20agent%20files) Custom agent handoff (first impressions) : r/GithubCopilot

[https://www.reddit.com/r/GithubCopilot/comments/1ojemfh/custom\_agent\_handoff\_first\_impressions/](https://www.reddit.com/r/GithubCopilot/comments/1ojemfh/custom_agent_handoff_first_impressions/)

[\[72\]](https://docs.github.com/en/copilot/how-tos/use-copilot-agents/coding-agent/create-custom-agents#:~:text=The%20following%20examples%20demonstrate%20what,copilot%20community%20collection) Creating custom agents \- GitHub Docs

[https://docs.github.com/en/copilot/how-tos/use-copilot-agents/coding-agent/create-custom-agents](https://docs.github.com/en/copilot/how-tos/use-copilot-agents/coding-agent/create-custom-agents)
